# Optimization Techniques
This repository will comprise primary optimization algorithms implementations in Python language. These algorithms based on Gradient Descent Algorithm.

## Algorithms
1. [Gradient Descent Algorithm](https://github.com/fzehracetin/optimization-techniques/blob/master/GradientDescent2D.ipynb)
2. [Steepest Descent Algorithm](https://github.com/fzehracetin/optimization-techniques/blob/master/SteepestDescent.ipynb)
3. [Gradient Descent with Momentum Algorithm](https://github.com/fzehracetin/optimization-techniques/blob/master/GradientDescentwithMomentum.ipynb)
4. [RMSprop Algorithm](https://github.com/fzehracetin/optimization-techniques/blob/master/RMSprop.ipynb)
5. [Adam Optimization Algorithm](https://github.com/fzehracetin/optimization-techniques/blob/master/Adam.ipynb)

## Framework


### Resources
I put useful links below to learn Optimization Techniques.

#### Articles
1. [Optimizers Explained - Adam, Momentum and Stochastic Gradient Descent](https://mlfromscratch.com/optimizers-explained/#/)
2. [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)
3. [Intro to optimization in deep learning: Momentum, RMSProp and Adam](https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/)
4. [Gradient Descent â€” Intro and Implementation in python](https://medium.com/analytics-vidhya/gradient-descent-intro-and-implementation-in-python-8b6ab0557b7c)

#### Videos
1. [Applied Optimization - Steepest Descent](https://www.youtube.com/watch?v=BBlDWNTimoA&feature=youtu.be)
2. [Gradient Descent With Momentum (C2W2L06)](https://www.youtube.com/watch?v=k8fTYJPd3_I&t=498s)
3. [RMSProp (C2W2L07)](https://www.youtube.com/watch?v=_e-LFe_igno&list=WL&index=26&t=0s)
4. [Adam Optimization Algorithm (C2W2L08)](https://www.youtube.com/watch?v=JXQT_vxqwIs)


